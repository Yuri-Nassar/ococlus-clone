{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.2\n"
     ]
    }
   ],
   "source": [
    "# import sklearn\n",
    "# print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pickle\n",
    "# import omega_index_py3 as oi\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import os, sys, shutil\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each algorithm has its block to facilitate the manipulation of it if the reader wants. Thus, feel free to change it to fit your needs. We ran the CORALS algorithm (it is based on MatLab) apart and computed the scores in this file. You can compute the reconstruction error from his code just by making simple adaptations on his project. Please, check Li's paper to get access to his project code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for traditional clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to compute the reconstruction error\n",
    "syn1 = './data/synthetic/preprocessed/Toy1_b4_binary_fortraditional.dat'\n",
    "syn2 = './data/synthetic/preprocessed/Toy2_b5_binary_fortraditional.dat'\n",
    "syn3 = './data/synthetic/preprocessed/Toy3_b2_binary_fortraditional.dat'\n",
    "files_reconstruct = [syn1, syn2, syn3]\n",
    "\n",
    "file1 = './data/synthetic/preprocessed/Toy1_b4_binary_fortraditional.dat'#synthetic-1\n",
    "file2 = './data/synthetic/preprocessed/Toy2_b5_binary_fortraditional.dat'#synthetic-2\n",
    "file3 = './data/synthetic/preprocessed/Toy3_b2_binary_fortraditional.dat'#synthetic-3\n",
    "datasets = [file1, file2, file3]\n",
    "clusters = [6,7,3] # number of clusters that traditinal clustering found in the ground-truth, respectively.\n",
    "numberOfRuns = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yurin\\Documents\\my_github\\ococlus\n",
      "kmeans' folder was cleaned.\n",
      "\n",
      "Dataset: Syn-1\n",
      "Run-1\n",
      "Reconstruction error:  6762\n",
      "\n",
      "Dataset: Syn-2\n",
      "Run-1\n",
      "Reconstruction error:  368875\n",
      "\n",
      "Dataset: Syn-3\n",
      "Run-1\n",
      "Reconstruction error:  3800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "path_method = \"OutputAnalysis\\kmeans\"\n",
    "check_path(path_method)\n",
    "\n",
    "for ds in range(len(datasets)):\n",
    "    ds_name = \"Syn-\"+str(ds+1)\n",
    "    print(\"\\nDataset: \"+ds_name)\n",
    "    res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "    for run in range(numberOfRuns):\n",
    "        print(\"Run-\"+str(run+1))\n",
    "        df = pd.read_csv(datasets[ds],header=None)\n",
    "        df.columns = [str(i) for i in range(df.shape[1])]\n",
    "        ncols = df.shape[1]\n",
    "        data = df.values.copy()\n",
    "        del df\n",
    "\n",
    "#         kmeans = KMeans(n_clusters = clusters[ds], random_state = 0)\n",
    "        kmeans = KMeans(n_clusters = clusters[ds])\n",
    "        kmeans.fit(data)\n",
    "        ids_clus = list(set(kmeans.labels_))\n",
    "        reconstructed_matrix = np.ones(data.shape,dtype=int)\n",
    "#         print(\"Data cost: \",data.sum())\n",
    "        print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_matrix)))\n",
    "        del data, reconstructed_matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        clustering = build_clustering_output_omega(ids_clus,(kmeans.labels_,ncols),trad=True)\n",
    "\n",
    "        # XMEASURES format ground-truth C++ version\n",
    "        kmeans_clustering_xm = xmeasures_format(clustering)\n",
    "        df_gt = pd.DataFrame(kmeans_clustering_xm)\n",
    "#         name = datasets[ds].split(\"/\")[-1]\n",
    "#         name = name.split(\".\")[0]\n",
    "        path = path_method+\"/\"+ds_name\n",
    "        df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_kmeans_\"+ds_name+\"_trad.cnl\", \n",
    "                     header= False,index=False, encoding='utf8')\n",
    "        del clustering, df_gt, kmeans_clustering_xm\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists. Clean it.\n",
      " \n",
      "File OutputAnalysis/kmeans/Syn-3/run_1_res_kmeans_Syn-3_trad.cnl is not empty. It has 3 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.44036 (Prc: 0.41129, Rec: 0.473852)\n",
      "OI:\n",
      "0.0623153\n",
      "MF1p_u: 0.44036 (Prc: 0.41129, Rec: 0.473852); OI: 0.0623153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 7000 nodes with hash 13278174547081972885, size: 7000, ids: 8209015500, id2s: 15763208793764);  synchronize: no, label: no\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score from Xmeasures project\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/kmeans\" ] \n",
    "then\n",
    "    echo \"Directory exists. Clean it.\"\n",
    "    rm -R xmeasures/OutputAnalysis/kmeans\n",
    "else\n",
    "    echo \"Directory does not exists. Create it now.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Methods: [Kmeans]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl, gt_xm_s2_trad.cnl, gt_xm_s3_trad.cnl]\n",
    "# for i in 1 2 3\n",
    "for i in 3\n",
    "do\n",
    "    for file in OutputAnalysis/kmeans/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/kmeans/Syn-3/run_1_res_kmeans_Syn-3_trad.cnl is not empty. It has 3 lines.\n",
      " \n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_trad.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/kmeans/Syn-3/run_1_res_kmeans_Syn-3_trad.cnl': 0.6812 (7000 / 10276)\n",
      "0.153246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 7000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI from Xmeasures project\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [kmeans]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl,gt_xm_s2_trad.cnl,gt_xm_s3_trad.cnl]\n",
    "# for i in 1 2 3\n",
    "for i in 3\n",
    "do\n",
    "    for file in OutputAnalysis/kmeans/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yurin\\Documents\\my_github\\ococlus\n",
      "The path was created: OutputAnalysis\\dbscan\n",
      "\n",
      "Dataset: Syn-1\n",
      "Run-1\n",
      "Reconstruction error:  6762\n",
      "\n",
      "Dataset: Syn-2\n",
      "Run-1\n",
      "Reconstruction error:  368875\n",
      "\n",
      "Dataset: Syn-3\n",
      "Run-1\n",
      "Reconstruction error:  3800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "path_method = \"OutputAnalysis\\dbscan\"\n",
    "check_path(path_method)\n",
    "for ds in range(len(datasets)):\n",
    "    ds_name = \"Syn-\"+str(ds+1)\n",
    "    print(\"\\nDataset: \"+ds_name)\n",
    "    res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "    for run in range(numberOfRuns):\n",
    "        print(\"Run-\"+str(run+1))\n",
    "    \n",
    "        df = pd.read_csv(datasets[ds],header=None)\n",
    "        df.columns = [str(i) for i in range(df.shape[1])]\n",
    "        ncols = df.shape[1]\n",
    "        data = df.values.copy()\n",
    "        del df\n",
    "\n",
    "        dbscan = DBSCAN(eps = 0.5, min_samples = 5) # default parameters\n",
    "        dbscan.fit(data)\n",
    "\n",
    "        ids_clus = [ele for ele in list(set(dbscan.labels_)) if ele != -1 ]\n",
    "\n",
    "        reconstructed_matrix = np.zeros(data.shape,dtype=int)\n",
    "        for i in range(len(dbscan.labels_)):\n",
    "            if dbscan.labels_[i] != -1: #mark the elements if it's not noise\n",
    "                reconstructed_matrix[i,:] = 1\n",
    "#         print(\"Data cost: \",data.sum())\n",
    "        print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_matrix)))\n",
    "        del data, reconstructed_matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        clustering = build_clustering_output_omega(ids_clus,(dbscan.labels_,ncols),trad=True)\n",
    "\n",
    "        # XMEASURES format ground-truth C++ version\n",
    "        dbscan_clustering_xm = xmeasures_format(clustering)\n",
    "        df_gt = pd.DataFrame(dbscan_clustering_xm)\n",
    "        path = path_method+\"/\"+ds_name\n",
    "        df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_dbscan_\"+ds_name+\"_trad.cnl\", \n",
    "                     header= False,index=False, encoding='utf8')\n",
    "        del clustering, df_gt, dbscan_clustering_xm\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory does not exists. Create it now.\n",
      " \n",
      "File OutputAnalysis/dbscan/Syn-3/run_1_res_dbscan_Syn-3_trad.cnl is not empty. It has 3 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.44036 (Prc: 0.41129, Rec: 0.473852)\n",
      "OI:\n",
      "0.0623153\n",
      "MF1p_u: 0.44036 (Prc: 0.41129, Rec: 0.473852); OI: 0.0623153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 7000 nodes with hash 13278174547081972885, size: 7000, ids: 8209015500, id2s: 15763208793764);  synchronize: no, label: no\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score from Xmeasures project\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/dbscan\" ] \n",
    "then\n",
    "    echo \"Directory exists. Clean it.\"\n",
    "    rm -R xmeasures/OutputAnalysis/dbscan\n",
    "else\n",
    "    echo \"Directory does not exists. Create it now.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Methods: [Kmeans]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl, gt_xm_s2_trad.cnl, gt_xm_s3_trad.cnl]\n",
    "# for i in 1 2 3\n",
    "for i in 3\n",
    "do\n",
    "    for file in OutputAnalysis/dbscan/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/dbscan/Syn-3/run_1_res_dbscan_Syn-3_trad.cnl is not empty. It has 3 lines.\n",
      " \n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_trad.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/dbscan/Syn-3/run_1_res_dbscan_Syn-3_trad.cnl': 0.6812 (7000 / 10276)\n",
      "0.153246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 7000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI from Xmeasures project\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [kmeans]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl,gt_xm_s2_trad.cnl,gt_xm_s3_trad.cnl]\n",
    "# for i in 1 2 3\n",
    "for i in 3\n",
    "do\n",
    "    for file in OutputAnalysis/dbscan/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierachical clustering (Agglomerative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yurin\\Documents\\my_github\\ococlus\n",
      "The path was created: OutputAnalysis\\Agglomerative\n",
      "\n",
      "Dataset: Syn-1\n",
      "Run-1\n",
      "Reconstruction error:  6762\n",
      "\n",
      "Dataset: Syn-2\n",
      "Run-1\n",
      "Reconstruction error:  368875\n",
      "\n",
      "Dataset: Syn-3\n",
      "Run-1\n",
      "Reconstruction error:  3800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "path_method = \"OutputAnalysis\\Agglomerative\"\n",
    "check_path(path_method)\n",
    "for ds in range(len(datasets)):\n",
    "    ds_name = \"Syn-\"+str(ds+1)\n",
    "    print(\"\\nDataset: \"+ds_name)\n",
    "    res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "    for run in range(numberOfRuns):\n",
    "        print(\"Run-\"+str(run+1))\n",
    "        \n",
    "        df = pd.read_csv(datasets[ds],header=None)\n",
    "        df.columns = [str(i) for i in range(df.shape[1])]\n",
    "        ncols = df.shape[1]\n",
    "        data = df.values.copy()\n",
    "        del df\n",
    "\n",
    "        HClustering = AgglomerativeClustering(n_clusters=clusters[ds], linkage='ward') #default parameters\n",
    "        HClustering.fit(data)\n",
    "\n",
    "        reconstructed_matrix = np.ones(data.shape,dtype=int)\n",
    "        ids_clus = list(set(HClustering.labels_))\n",
    "#         print(\"Data cost: \",data.sum())\n",
    "        print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_matrix)))\n",
    "        del reconstructed_matrix, data\n",
    "        gc.collect()\n",
    "        \n",
    "        clustering = build_clustering_output_omega(ids_clus,(HClustering.labels_,ncols),trad=True)\n",
    "\n",
    "        # XMEASURES format ground-truth C++ version\n",
    "        HClustering_clustering_xm = xmeasures_format(clustering)\n",
    "        df_gt = pd.DataFrame(HClustering_clustering_xm)\n",
    "        path = path_method+\"/\"+ds_name\n",
    "        df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_agglomerative_\"+ds_name+\"_trad.cnl\", \n",
    "                     header= False,index=False, encoding='utf8')\n",
    "        del clustering, df_gt, HClustering_clustering_xm\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory does not exists. Create it now.\n",
      " \n",
      "File OutputAnalysis/Agglomerative/Syn-3/run_1_res_agglomerative_Syn-3_trad.cnl is not empty. It has 3 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.44036 (Prc: 0.41129, Rec: 0.473852)\n",
      "OI:\n",
      "0.0623153\n",
      "MF1p_u: 0.44036 (Prc: 0.41129, Rec: 0.473852); OI: 0.0623153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 7000 nodes with hash 13278174547081972885, size: 7000, ids: 8209015500, id2s: 15763208793764);  synchronize: no, label: no\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score from Xmeasures project\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/Agglomerative\" ] \n",
    "then\n",
    "    echo \"Directory exists. Clean it.\"\n",
    "    rm -R xmeasures/OutputAnalysis/Agglomerative\n",
    "else\n",
    "    echo \"Directory does not exists. Create it now.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Methods: [Kmeans]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl, gt_xm_s2_trad.cnl, gt_xm_s3_trad.cnl]\n",
    "# for i in 1 2 3\n",
    "for i in 3\n",
    "do\n",
    "    for file in OutputAnalysis/Agglomerative/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/Agglomerative/Syn-3/run_1_res_agglomerative_Syn-3_trad.cnl is not empty. It has 3 lines.\n",
      " \n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_trad.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Agglomerative/Syn-3/run_1_res_agglomerative_Syn-3_trad.cnl': 0.6812 (7000 / 10276)\n",
      "0.153246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 7000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI from Xmeasures project\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [kmeans]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl,gt_xm_s2_trad.cnl,gt_xm_s3_trad.cnl]\n",
    "# for i in 1 2 3\n",
    "for i in 3\n",
    "do\n",
    "    for file in OutputAnalysis/Agglomerative/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for co-clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn1 = './data/synthetic/preprocessed/Toy1_b4_binary_forcocluster.dat'\n",
    "syn2 = './data/synthetic/preprocessed/Toy2_b5_binary_forcocluster.dat'\n",
    "syn3 = './data/synthetic/preprocessed/Toy3_b2_binary_forcocluster.dat'\n",
    "files_reconstruct = [syn1, syn2, syn3]\n",
    "\n",
    "file1 = './data/synthetic/preprocessed/Toy1_b4_binary_forcocluster.dat'#synthetic-1\n",
    "file2 = './data/synthetic/preprocessed/Toy2_b5_binary_forcocluster.dat'#synthetic-2\n",
    "file3 = './data/synthetic/preprocessed/Toy3_b2_binary_forcocluster.dat'#synthetic-3\n",
    "datasets = [file1, file2, file3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-clustering (Block diagonal) - Dhillon (2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yurin\\Documents\\my_github\\ococlus\n",
      "dhillon' folder was cleaned.\n",
      "\n",
      "Dataset: Syn-1\n",
      "Run-1\n",
      "Reconstruction error:  1404\n",
      "\n",
      "Dataset: Syn-2\n",
      "Run-1\n",
      "Reconstruction error:  31426\n",
      "\n",
      "Dataset: Syn-3\n",
      "Run-1\n",
      "Reconstruction error:  2200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "clusters = [7,10,4] # number of co-clusters in the ground-truth data, respectively.\n",
    "path_method = \"OutputAnalysis\\dhillon\"\n",
    "check_path(path_method)\n",
    "for ds in range(len(datasets)):\n",
    "    ds_name = \"Syn-\"+str(ds+1)\n",
    "    print(\"\\nDataset: \"+ds_name)\n",
    "    res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "    for run in range(numberOfRuns):\n",
    "        print(\"Run-\"+str(run+1))\n",
    "        \n",
    "        df = pd.read_csv(datasets[ds],header=None)\n",
    "        df.columns = [i+1 for i in range(df.shape[1])]\n",
    "        data = df.values.copy()\n",
    "        del df\n",
    "\n",
    "        DhillonCocluster = SpectralCoclustering(n_clusters = clusters[ds])\n",
    "        DhillonCocluster.fit(data)\n",
    "\n",
    "        # Reconstruct matrix\n",
    "        reconstructed_matrix = np.zeros(data.shape,dtype=int)\n",
    "        for nc in range(clusters[ds]):\n",
    "            if len(DhillonCocluster.get_indices(nc)[0]) != 0 and len(DhillonCocluster.get_indices(nc)[1]) != 0:\n",
    "                for i in DhillonCocluster.get_indices(nc)[0]:\n",
    "                    for j in DhillonCocluster.get_indices(nc)[1]:\n",
    "                        reconstructed_matrix[i][j] = 1\n",
    "#         print(\"Data cost: \",data.sum())\n",
    "        print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_matrix)))\n",
    "        \n",
    "        clustering = build_clustering_output_omega(clusters[ds],DhillonCocluster) #format to omega and f-score measure\n",
    "\n",
    "        # XMEASURES format ground-truth C++ version\n",
    "        DhillonCocluster_clustering_xm = xmeasures_format(clustering)\n",
    "        df_gt = pd.DataFrame(DhillonCocluster_clustering_xm)\n",
    "        path = path_method+\"/\"+ds_name\n",
    "        df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_dhillon_\"+ds_name+\"_co.cnl\", \n",
    "                     header= False,index=False, encoding='utf8')\n",
    "        del reconstructed_matrix, clustering, df_gt, DhillonCocluster_clustering_xm\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory does not exists. Create it now.\n",
      " \n",
      "File OutputAnalysis/dhillon/Syn-3/run_1_res_dhillon_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.385612 (Prc: 0.325192, Rec: 0.473607)\n",
      "OI:\n",
      "-0.0122583\n",
      "MF1p_u: 0.385612 (Prc: 0.325192, Rec: 0.473607); OI: -0.0122583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 1000 nodes with hash 7130370568895277204, size: 1000, ids: 879620500, id2s: 2274756543100);  synchronize: no, label: no\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score from Xmeasures project\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/dhillon\" ] \n",
    "then\n",
    "    echo \"Directory exists. Clean it.\"\n",
    "    rm -R xmeasures/OutputAnalysis/dhillon\n",
    "else\n",
    "    echo \"Directory does not exists. Create it now.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Methods: [Dhillon]\n",
    "#ground-truth: [gt_xm_s1_co.cnl, gt_xm_s2_co.cnl, gt_xm_s3_co.cnl]\n",
    "# for i in 1 2 3 # checking all datasets\n",
    "for i in 3 # checking a specific dataset\n",
    "do\n",
    "    for file in OutputAnalysis/dhillon/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_co.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/dhillon/Syn-3/run_1_res_dhillon_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/dhillon/Syn-3/run_1_res_dhillon_Syn-3_co.cnl': 0.5711 (1000 / 1751)\n",
      "0.156452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 1000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI from Xmeasures project\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [Dhillon]\n",
    "#ground-truth: [gt_xm_s1_co.cnl,gt_xm_s2_co.cnl,gt_xm_s3_co.cnl]\n",
    "# for i in 1 2 3 # checking all datasets\n",
    "for i in 3 # checking a specific dataset\n",
    "do\n",
    "    for file in OutputAnalysis/dhillon/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_co.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-clustering (Checkerboard) - Kluger (2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yurin\\Documents\\my_github\\ococlus\n",
      "kluger' folder was cleaned.\n",
      "\n",
      "Dataset: Syn-1\n",
      "Run-1\n",
      "Reconstruction error:  3759\n",
      "\n",
      "Dataset: Syn-2\n",
      "Run-1\n",
      "Reconstruction error:  343825\n",
      "\n",
      "Dataset: Syn-3\n",
      "Run-1\n",
      "Reconstruction error:  1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster.bicluster import SpectralBiclustering\n",
    "clusters = [(6,5),(7,8),(3,3)] # number of co-clusters in the ground-truth data, respectively.\n",
    "path_method = \"OutputAnalysis\\kluger\"\n",
    "check_path(path_method)\n",
    "for ds in range(len(datasets)):\n",
    "    ds_name = \"Syn-\"+str(ds+1)\n",
    "    print(\"\\nDataset: \"+ds_name)\n",
    "    res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "    for run in range(numberOfRuns):\n",
    "        print(\"Run-\"+str(run+1))\n",
    "        \n",
    "        df = pd.read_csv(datasets[ds],header=None)\n",
    "        df.columns = [i+1 for i in range(df.shape[1])]\n",
    "        data = df.values.copy()\n",
    "        del df\n",
    "\n",
    "        KlugerCocluster = SpectralBiclustering(n_clusters = clusters[ds])\n",
    "        KlugerCocluster.fit(data)\n",
    "\n",
    "        reconstructed_kluger = np.zeros(data.shape,dtype=int)\n",
    "        for nc in range((clusters[ds][0]*clusters[ds][1])):\n",
    "            if len(KlugerCocluster.get_indices(nc)[0]) != 0 and len(KlugerCocluster.get_indices(nc)[1]) != 0:\n",
    "                for i in KlugerCocluster.get_indices(nc)[0]:\n",
    "                    for j in KlugerCocluster.get_indices(nc)[1]:\n",
    "                        reconstructed_kluger[i][j] = 1\n",
    "        print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_kluger)))\n",
    "        del reconstructed_kluger, data\n",
    "        gc.collect()\n",
    "        \n",
    "        clustering = build_clustering_output_omega(clusters[ds],KlugerCocluster) #format to omega and f-score measure\n",
    "\n",
    "        # XMEASURES format ground-truth C++ version\n",
    "        KlugerCocluster_clustering_xm = xmeasures_format(clustering)\n",
    "        df_gt = pd.DataFrame(KlugerCocluster_clustering_xm)\n",
    "        path = path_method+\"/\"+ds_name\n",
    "        df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_kluger_\"+ds_name+\"_co.cnl\", \n",
    "                     header= False,index=False, encoding='utf8')\n",
    "        del clustering, df_gt, KlugerCocluster_clustering_xm\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory does not exists. Create it now.\n",
      " \n",
      "File OutputAnalysis/kluger/Syn-3/run_1_res_kluger_Syn-3_co.cnl is not empty. It has 9 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.457622 (Prc: 0.580247, Rec: 0.377784)\n",
      "OI:\n",
      "0.0834991\n",
      "MF1p_u: 0.457622 (Prc: 0.580247, Rec: 0.377784); OI: 0.0834991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 4200 nodes with hash 14672793821654551294, size: 4200, ids: 4618734900, id2s: 9532372343452);  synchronize: no, label: no\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score from Xmeasures project\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/kluger\" ] \n",
    "then\n",
    "    echo \"Directory exists. Clean it.\"\n",
    "    rm -R xmeasures/OutputAnalysis/kluger\n",
    "else\n",
    "    echo \"Directory does not exists. Create it now.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Methods: [Kluger]\n",
    "#ground-truth: [gt_xm_s1_co.cnl, gt_xm_s2_co.cnl, gt_xm_s3_co.cnl]\n",
    "# for i in 1 2 3 # checking all datasets\n",
    "for i in 3 # checking a specific dataset\n",
    "do\n",
    "    for file in OutputAnalysis/kluger/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_co.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/kluger/Syn-3/run_1_res_kluger_Syn-3_co.cnl is not empty. It has 9 lines.\n",
      " \n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/kluger/Syn-3/run_1_res_kluger_Syn-3_co.cnl': 0.6636 (4200 / 6329)\n",
      "0.293504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 4200\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI from Xmeasures project\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [kluger]\n",
    "#ground-truth: [gt_xm_s1_co.cnl,gt_xm_s2_co.cnl,gt_xm_s3_co.cnl]\n",
    "# for i in 1 2 3 # checking all datasets\n",
    "for i in 3 # checking a specific dataset\n",
    "do\n",
    "    for file in OutputAnalysis/kluger/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_co.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Li method (CORALS) - 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show below the evaluation scores using the xmeasures project regarding the clustering results of Li's project code. You can compute the reconstruction error by making a simple modification on his code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory does not exists. Create it now.\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_10_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_11_result_Li_Syn-3_co.cnl\n",
      "Empty file. SKIPPED!\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_12_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_13_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_14_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_15_result_Li_Syn-3_co.cnl\n",
      "Empty file. SKIPPED!\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_1_result_Li_Syn-3_co.cnl is not empty. It has 1 lines.\n",
      " \n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_2_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_3_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_4_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_5_result_Li_Syn-3_co.cnl is not empty. It has 1 lines.\n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.47631 (Prc: 0.42417, Rec: 0.543064)\n",
      "OI:\n",
      "0.068298\n",
      "MF1p_u: 0.47631 (Prc: 0.42417, Rec: 0.543064); OI: 0.068298\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_6_result_Li_Syn-3_co.cnl is not empty. It has 1 lines.\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.458118 (Prc: 0.403078, Rec: 0.530566)\n",
      "OI:\n",
      "0.0499853\n",
      "MF1p_u: 0.458118 (Prc: 0.403078, Rec: 0.530566); OI: 0.0499853\n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.467857 (Prc: 0.415443, Rec: 0.535407)\n",
      "OI:\n",
      "0.00492635\n",
      "MF1p_u: 0.467857 (Prc: 0.415443, Rec: 0.535407); OI: 0.00492635\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.345251 (Prc: 0.256176, Rec: 0.529289)\n",
      "OI:\n",
      "-0.0605261\n",
      "MF1p_u: 0.345251 (Prc: 0.256176, Rec: 0.529289); OI: -0.0605261\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_7_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.485838 (Prc: 0.427015, Rec: 0.563455)\n",
      "OI:\n",
      "0.0827662\n",
      "MF1p_u: 0.485838 (Prc: 0.427015, Rec: 0.563455); OI: 0.0827662\n",
      "File OutputAnalysis/Li/Syn-3/run_8_result_Li_Syn-3_co.cnl\n",
      "Empty file. SKIPPED!\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_9_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.408467 (Prc: 0.366605, Rec: 0.461122)\n",
      "OI:\n",
      "-0.0061138\n",
      "MF1p_u: 0.408467 (Prc: 0.366605, Rec: 0.461122); OI: -0.0061138\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.454918 (Prc: 0.401172, Rec: 0.525293)\n",
      "OI:\n",
      "0.0283126\n",
      "MF1p_u: 0.454918 (Prc: 0.401172, Rec: 0.525293); OI: 0.0283126\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.443371 (Prc: 0.390917, Rec: 0.512083)\n",
      "OI:\n",
      "0.0231114\n",
      "MF1p_u: 0.443371 (Prc: 0.390917, Rec: 0.512083); OI: 0.0231114\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.365563 (Prc: 0.286607, Rec: 0.50456)\n",
      "OI:\n",
      "-0.0771869\n",
      "MF1p_u: 0.365563 (Prc: 0.286607, Rec: 0.50456); OI: -0.0771869\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.345134 (Prc: 0.256712, Rec: 0.526475)\n",
      "OI:\n",
      "-0.0626809\n",
      "MF1p_u: 0.345134 (Prc: 0.256712, Rec: 0.526475); OI: -0.0626809\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.500507 (Prc: 0.439095, Rec: 0.581888)\n",
      "OI:\n",
      "0.118557\n",
      "MF1p_u: 0.500507 (Prc: 0.439095, Rec: 0.581888); OI: 0.118557\n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "0.447337 (Prc: 0.388325, Rec: 0.527499)\n",
      "OI:\n",
      "0.0582723\n",
      "MF1p_u: 0.447337 (Prc: 0.388325, Rec: 0.527499); OI: 0.0582723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3551 nodes with hash 7772750386298721386, size: 3551, ids: 4246892752, id2s: 8040628001876);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3257 nodes with hash 5515070431265528069, size: 3257, ids: 4019459001, id2s: 7449489215973);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3489 nodes with hash 8179208330225327641, size: 3489, ids: 3940645836, id2s: 7739001506348);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3968 nodes with hash 10303997184495713954, size: 3968, ids: 4925987325, id2s: 8736616158069);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 2236 nodes with hash 11433030750653366122, size: 2236, ids: 2333283740, id2s: 4858752215644);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 2687 nodes with hash 7709722172838418067, size: 2687, ids: 3033307041, id2s: 6057498799241);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3395 nodes with hash 15180725106544979757, size: 3395, ids: 4194448235, id2s: 7600565528107);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3601 nodes with hash 9528649368861483942, size: 3601, ids: 4340642395, id2s: 8476055722799);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 2365 nodes with hash 2403985157141696871, size: 2365, ids: 2515748365, id2s: 5300265880511);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 2162 nodes with hash 14740577181103666352, size: 2162, ids: 2101002280, id2s: 5005194339484);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 3508 nodes with hash 16557232160085772579, size: 3508, ids: 4187447916, id2s: 7885010436912);  synchronize: no, label: no\n",
      "WARNING, the nodes in the collections differ (the quality will be penalized): 3200 nodes with hash 16332818011922301848, size: 3200, ids: 3695102400, id2s: 7185057752512) != 2430 nodes with hash 8468759859296573804, size: 2430, ids: 2800876270, id2s: 5198291292312);  synchronize: no, label: no\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score from Xmeasures project\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/Li\" ] \n",
    "then\n",
    "    echo \"Directory exists. Clean it.\"\n",
    "    rm -R xmeasures/OutputAnalysis/Li\n",
    "else\n",
    "    echo \"Directory does not exists. Create it now.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Methods: [Li]\n",
    "#ground-truth: [gt_xm_s1_co.cnl,gt_xm_s2_co.cnl, gt_xm_s3_co.cnl]\n",
    "# for i in 1 2 3 # checking all datasets\n",
    "for i in 3 # checking a specific dataset\n",
    "do\n",
    "    for file in OutputAnalysis/Li/Syn-${i}/*\n",
    "    do\n",
    "        res=$(grep -c '' ${file})\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_co.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_10_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_10_result_Li_Syn-3_co.cnl': 0.664 (3971 / 5980)\n",
      "0.0613068\n",
      "File OutputAnalysis/Li/Syn-3/run_11_result_Li_Syn-3_co.cnl\n",
      "Empty file. SKIPPED!\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_12_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_12_result_Li_Syn-3_co.cnl': 0.6728 (3665 / 5447)\n",
      "0.046774\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_13_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_13_result_Li_Syn-3_co.cnl': 0.6721 (3831 / 5700)\n",
      "0.0863685\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_14_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_14_result_Li_Syn-3_co.cnl': 0.6825 (4568 / 6693)\n",
      "0.0281267\n",
      "File OutputAnalysis/Li/Syn-3/run_15_result_Li_Syn-3_co.cnl\n",
      "Empty file. SKIPPED!\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_1_result_Li_Syn-3_co.cnl is not empty. It has 1 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_1_result_Li_Syn-3_co.cnl': 0.6304 (2236 / 3547)\n",
      "0.00776087\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_2_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_2_result_Li_Syn-3_co.cnl': 0.6557 (2925 / 4461)\n",
      "0.0717442\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_3_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_3_result_Li_Syn-3_co.cnl': 0.6669 (3752 / 5626)\n",
      "0.0684405\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_4_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_4_result_Li_Syn-3_co.cnl': 0.6622 (4057 / 6127)\n",
      "0.0471182\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_5_result_Li_Syn-3_co.cnl is not empty. It has 1 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_5_result_Li_Syn-3_co.cnl': 0.6849 (2365 / 3453)\n",
      "0.00500104\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_6_result_Li_Syn-3_co.cnl is not empty. It has 1 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_6_result_Li_Syn-3_co.cnl': 0.6357 (2162 / 3401)\n",
      "0.00756226\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_7_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_7_result_Li_Syn-3_co.cnl': 0.6735 (3832 / 5690)\n",
      "0.0545381\n",
      "File OutputAnalysis/Li/Syn-3/run_8_result_Li_Syn-3_co.cnl\n",
      "Empty file. SKIPPED!\n",
      " \n",
      "File OutputAnalysis/Li/Syn-3/run_9_result_Li_Syn-3_co.cnl is not empty. It has 2 lines.\n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s3_co.cnl': 0.6696 (6200 / 9259)\n",
      "# Average estimated membership in 'OutputAnalysis/Li/Syn-3/run_9_result_Li_Syn-3_co.cnl': 0.667 (2580 / 3868)\n",
      "0.10826\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3551\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3257\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3489\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3968\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 2236\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 2687\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3395\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3601\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 2365\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 2162\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 3508\n",
      "WARNING, the number of nodes is different in the collections (the quality will be penalized): 3200 != 2430\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI from Xmeasures project\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Methods: [Li]\n",
    "#ground-truth: [gt_xm_s1_co.cnl, gt_xm_s2_co.cnl, gt_xm_s3_co.cnl]\n",
    "# for i in 1 2 3 # checking all datasets\n",
    "for i in 3 # checking a specific dataset\n",
    "do\n",
    "    for file in OutputAnalysis/Li/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_co.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "    wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "#     wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check path results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path_method):\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "    method_name = path_method.split(\"\\\\\")[1]\n",
    "#     print(method_name)\n",
    "    res = os.path.exists(path_method)\n",
    "    # clean the folder to save new data\n",
    "    if res:\n",
    "        #check if it is empty\n",
    "        dir_empty = os.listdir(path_method)\n",
    "        if len(dir_empty) != 0:\n",
    "    #         shutil.rmtree(\"OutputAnalysis/kmeans/\")\n",
    "            rm = !rm -r --preserve-root './OutputAnalysis/'{method_name}'/'*\n",
    "            if not rm:\n",
    "                print(method_name+\"' folder was cleaned.\")\n",
    "    #             os.chdir(path_method)\n",
    "            else:\n",
    "                print(\"sad\")\n",
    "                print(rm)\n",
    "        else:\n",
    "            print(\"This folder is Empty!\")\n",
    "            pass\n",
    "    #         os.chdir(path_method)\n",
    "    else: # nothing exist so create it\n",
    "        # trying to insert to flase directory \n",
    "        try: \n",
    "    #         os.chdir(fd) \n",
    "            os.mkdir(path_method)\n",
    "            print(\"The path was created: \"+path_method)\n",
    "\n",
    "        # Caching the exception     \n",
    "        except: \n",
    "            print(\"Something wrong with specified directory. Exception- \", sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format result to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clustering_output_omega(clusters,clus_labels,trad=False):\n",
    "    # build the clustering output format to use in the omega index evaluation\n",
    "    clustering = {}\n",
    "    num_of_elements = 0\n",
    "    \n",
    "    if trad:\n",
    "        for cluster_id in clusters:\n",
    "            clustering[\"c\"+str(cluster_id)] = []\n",
    "            for index, value in enumerate(clus_labels[0]):#looping over the labels\n",
    "                if cluster_id == value:\n",
    "                    [clustering[\"c\"+str(cluster_id)].append((\"01\"+str(index)+\"02\"+str(j))) for j in range(clus_labels[1])]# num of attributes\n",
    "            num_of_elements+=len(clustering[\"c\"+str(cluster_id)])\n",
    "    else:\n",
    "        if isinstance(clusters,int):\n",
    "            num = clusters\n",
    "        else:\n",
    "            num = (clusters[0]*clusters[1])\n",
    "\n",
    "        seq_index = 0\n",
    "        for nc in range(num):\n",
    "             ## check if exist some co-cluster with no element assignment for object and attribute\n",
    "#             if len(clus_labels.get_indices(nc)[0]) != 0 and len(clus_labels.get_indices(nc)[1]) != 0:\n",
    "#                 clustering[\"c\"+str(seq_index)] = []\n",
    "#                 for i in clus_labels.get_indices(nc)[0]:\n",
    "#                     for j in clus_labels.get_indices(nc)[1]:\n",
    "#                         clustering[\"c\"+str(seq_index)].append((\"01\"+str(i)+\"02\"+str(j)))\n",
    "#             elif len(clus_labels.get_indices(nc)[0]) != 0 and len(clus_labels.get_indices(nc)[1]) == 0:\n",
    "#                 clustering[\"c\"+str(seq_index)] = []\n",
    "#                 for i in clus_labels.get_indices(nc)[0]:\n",
    "#                     clustering[\"c\"+str(seq_index)].append(str(i))\n",
    "#             elif len(clus_labels.get_indices(nc)[1]) != 0 and len(clus_labels.get_indices(nc)[0]) == 0:\n",
    "#                 clustering[\"c\"+str(seq_index)] = []\n",
    "#                 for j in clus_labels.get_indices(nc)[1]:\n",
    "#                     clustering[\"c\"+str(seq_index)].append(str(j))\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "#             num_of_elements += len(clustering[\"c\"+str(seq_index)])\n",
    "#             seq_index += 1\n",
    "            \n",
    "            \n",
    "            ## check if exist some co-cluster with no element assignment for object and attribute\n",
    "            if len(clus_labels.get_indices(nc)[0]) != 0 and len(clus_labels.get_indices(nc)[1]) != 0:\n",
    "                clustering[\"c\"+str(seq_index)] = []\n",
    "                for i in clus_labels.get_indices(nc)[0]:\n",
    "                    for j in clus_labels.get_indices(nc)[1]:\n",
    "                        clustering[\"c\"+str(seq_index)].append((\"01\"+str(i)+\"02\"+str(j)))\n",
    "                num_of_elements += len(clustering[\"c\"+str(seq_index)])\n",
    "                seq_index+=1\n",
    "#         print(\"Number of elements in OMEGA format: \",num_of_elements)\n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmeasures_format(dict_gt):\n",
    "    newData = []\n",
    "#     print(len(dict_gt))\n",
    "    r = 0\n",
    "    for i in range(len(dict_gt)):\n",
    "#         print(i,end=\":\")\n",
    "        stringLine = str(dict_gt['c'+str(i)][0])\n",
    "#         print(len(dict_gt['c'+str(i)]),end=\",\")\n",
    "        r += len(dict_gt['c'+str(i)])\n",
    "        for j in range(1,len(dict_gt['c'+str(i)])):\n",
    "#             stringLine = stringLine+\" \"+dict_gt['c'+str(i)][j]\n",
    "            stringLine += \" \"+str(dict_gt['c'+str(i)][j])\n",
    "        newData.append(stringLine)\n",
    "#     print(\"Number of elements in XMEASURES format: \",r)\n",
    "    return newData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
